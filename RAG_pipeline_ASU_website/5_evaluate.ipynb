{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff548ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################   \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"silver_data.csv\")\n",
    "df = df[(df['section']!= 'References') & (df['section']!= 'Appendix') & (df['section']!='Disclaimer')]\n",
    "df = df.reset_index(drop=True)\n",
    "df['context'] = df['contex']\n",
    "\n",
    "df['question'] = df['question'].str.replace('\\n', '')\n",
    "df['answer'] = df['answer'].str.replace('\\n', '')\n",
    "df['context'] = df['context'].str.replace('\\n', '')\n",
    "\n",
    "df['similarity_score'] = 0.0\n",
    "df['BLEU'] = 0.0\n",
    "df['Cosine'] = 0.0\n",
    "\n",
    "df = df[['section', 'title', 'file_name', 'document_type', 'page', 'total_pages', 'context', 'question', 'answer', 'similarity_score', 'BLEU', 'Cosine']]\n",
    "\n",
    "\n",
    "#################################\n",
    "for i in range(0, len(df['question'])):\n",
    "    user_input = df['question'].iloc[i]\n",
    "    response = df['answer'].iloc[i]\n",
    "    retrieved_context = df['context'].iloc[i]\n",
    "\n",
    "    similarity_score = fun.calculate_fuzzy_similarity(response, retrieved_context)\n",
    "    df.loc[i, 'similarity_score'] = similarity_score\n",
    "    # print(f\"Fuzzy Similarity Score: {similarity_score}\")\n",
    "\n",
    "    tfidf_similarity = fun.calculate_tfidf_cosine_similarity(response, retrieved_context)\n",
    "    df.loc[i, 'Cosine'] = tfidf_similarity\n",
    "    # print(f\"TF-IDF Cosine Similarity: {tfidf_similarity:.2f}\")\n",
    "\n",
    "    bleu_score = fun.calculate_bleu_score(response, retrieved_context)\n",
    "    df.loc[i, 'BLEU'] = bleu_score\n",
    "    # print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "df.to_csv('silver_data_graded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ASU_key = os.environ.get(\"ASU_key\") \n",
    "endpoint_url = os.environ.get(\"endpoint_url\") \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {ASU_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "#############################################################################\n",
    "import pandas as pd\n",
    "# df = pd.read_csv('qa_results_evaluated_v2.csv')\n",
    "# df.head()\n",
    "# df['ragas_bleu'] = 0.0\n",
    "# df['ragas_precision_reference'] = 0.0\n",
    "# df['ragas_faith'] = 0.0\n",
    "# df['ragas_fact'] = 0.0\n",
    "# df['regas_sem_sim'] = 0.0\n",
    "# df['regas_rogue'] = 0.0\n",
    "start_ii = 1434\n",
    "if start_ii > 0:\n",
    "    df = pd.read_csv('qa_results_RAGAS_evaluated.csv')\n",
    "\n",
    "for ii in range(start_ii, 6635):\n",
    "    print(ii)\n",
    "    questiony = df['question'].iloc[ii]\n",
    "    answery = df['answer'].iloc[ii]\n",
    "    contexty = df['page_text'].iloc[ii]\n",
    "\n",
    "    ##################################\n",
    "    ### Bluescore\n",
    "    payload_blue = {\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"metric\": \"bleuscore\",\n",
    "        \"parameters\": {\n",
    "            \"user_input\": questiony,\n",
    "            \"response\": answery,\n",
    "            \"reference\": contexty,\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(endpoint_url, headers=headers, json=payload_blue)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(\"result:\", result)\n",
    "        #### SAVE BLEU score\n",
    "        df.loc[ii,'ragas_bleu'] = result['score']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    ##################################\n",
    "    ### Context Precision With Reference\n",
    "    # payload_precision_reference = {\n",
    "    #     \"model_provider\": \"openai\",\n",
    "    #     \"model_name\": \"gpt-4o\",\n",
    "    #     \"metric\": \"context_precision_with_reference\",\n",
    "    #     \"parameters\": {\n",
    "    #         \"user_input\": questiony,\n",
    "    #         \"reference\": answery,\n",
    "    #         \"retrieved_contexts\": [\n",
    "    #             contexty,\n",
    "    #         ]\n",
    "    #     }\n",
    "    # }\n",
    "    # try:\n",
    "    #     response = requests.post(endpoint_url, headers=headers, json=payload_precision_reference)\n",
    "    #     response.raise_for_status()\n",
    "    #     result = response.json()\n",
    "    #     print(\"result:\", result)\n",
    "    #     #### SAVE Precision With Reference score\n",
    "    #     df.loc[ii,'ragas_precision_reference'] = result['score']\n",
    "    # except requests.exceptions.RequestException as e:\n",
    "    #     print(f\"API request error: {e}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    ##################################\n",
    "    # ### RAGAS Faithfulness\n",
    "    # payload_ragas_faith = {\n",
    "    #     \"model_provider\": \"openai\",\n",
    "    #     \"model_name\": \"gpt-4o\",\n",
    "    #     \"metric\": \"faithfulness\",\n",
    "    #     \"parameters\": {\n",
    "    #         \"user_input\": questiony,\n",
    "    #         \"response\": answery,\n",
    "    #         \"retrieved_contexts\": [\n",
    "    #             contexty,\n",
    "    #         ]\n",
    "    #     }\n",
    "    # }\n",
    "    # try:\n",
    "    #     response = requests.post(endpoint_url, headers=headers, json=payload_ragas_faith)\n",
    "    #     response.raise_for_status()\n",
    "    #     result = response.json()\n",
    "    #     print(\"result:\", result)\n",
    "    #     #### SAVE RAGAS Faithfulness score\n",
    "    #     df.loc[ii,'ragas_faith'] = result['score']\n",
    "    # except requests.exceptions.RequestException as e:\n",
    "    #     print(f\"API request error: {e}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    ##################################\n",
    "    ### Factual correctness\n",
    "    # print(\"Factual Correctness\")\n",
    "    payload_fact = {\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"metric\": \"factual_correctness\",\n",
    "        \"parameters\": {\n",
    "            \"response\": str(answery),\n",
    "            \"reference\": str(contexty)\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(endpoint_url, headers=headers, json=payload_fact)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(\"result:\", result)\n",
    "        #### SAVE Factual correctness score\n",
    "        df.loc[ii,'ragas_fact'] = result['score']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    ##################################\n",
    "    ### Semantic similarity\n",
    "    payload_sem_sim = {\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"metric\": \"semantic_similarity\",\n",
    "        \"parameters\": {\n",
    "            \"response\": answery,\n",
    "            \"reference\": contexty\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(endpoint_url, headers=headers, json=payload_sem_sim)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(\"result:\", result)\n",
    "        #### SAVE Factual correctness score\n",
    "        df.loc[ii,'regas_sem_sim'] = result['score']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    ##################################\n",
    "    ### Rouge score\n",
    "    payload_rogue = {\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"metric\": \"rouge_score\",\n",
    "        \"parameters\": {\n",
    "            \"response\": answery,\n",
    "            \"reference\": contexty\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(endpoint_url, headers=headers, json=payload_rogue)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        print(\"result:\", result)\n",
    "        #### SAVE Factual correctness score\n",
    "        df.loc[ii,'regas_rogue'] = result['score']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "    df.to_csv('qa_results_RAGAS_evaluated.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
