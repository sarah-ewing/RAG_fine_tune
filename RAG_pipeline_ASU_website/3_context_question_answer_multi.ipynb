{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e054eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "num_threads = 3\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "file_name = os.environ.get(\"file_name\")\n",
    "directory_path = 'C:\\\\programming_projects\\\\RAG_fine_tune\\\\RAG_pipeline_ASU_website\\\\data\\\\'\n",
    "file_path = directory_path + 'chunked_' + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf62331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 start: 0 end: 100\n",
      "Thread 1: Starting at index 0\n",
      "1 start: 100 end: 200\n",
      "Thread 2: Starting at index 0\n",
      "2 start: 200 end: 300\n",
      "Thread 3: Starting at index 0\n",
      "Thread 2: Finished processing rows 0 to 99 and saved to thread_2_data_0.csv\n",
      "Thread 1: Finished processing rows 0 to 99 and saved to thread_1_data_0.csv\n",
      "Thread 3: Finished processing rows 0 to 99 and saved to thread_3_data_0.csv\n",
      "All threads have finished and saved their DataFrames.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def worker(thread_id, all_data, start_index, num_rows=100):\n",
    "    \"\"\"Worker function that processes a chunk of the DataFrame and saves to CSV.\"\"\"\n",
    "    print(f\"Thread {thread_id}: Starting at index {start_index}\")\n",
    "    end_index = start_index + num_rows\n",
    "    thread_data = all_data.iloc[start_index:end_index]  # Use .iloc for positional indexing\n",
    "    filename = f\"thread_{thread_id}_data_{start_index}.csv\"\n",
    "    thread_data.to_csv(filename, index=False)\n",
    "    print(f\"Thread {thread_id}: Finished processing rows {start_index} to {end_index - 1} and saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threads = []\n",
    "    num_threads = 3\n",
    "    rows_per_thread = 100\n",
    "\n",
    "    try:\n",
    "        all_data = pd.read_csv(file_path, nrows=400)\n",
    "        total_rows = len(all_data)\n",
    "        start_indices = [i * rows_per_thread for i in range(num_threads)]\n",
    "\n",
    "        for i in range(num_threads):\n",
    "            print(i, \"start:\", (i * 100), \"end:\", ((i+1) * 100))\n",
    "            start_index = (i * 100) ##start_indices[i]\n",
    "            end_index = start_index + rows_per_thread\n",
    "            if start_index < total_rows:\n",
    "                # Ensure we don't go beyond the total number of rows\n",
    "                actual_end_index = min(end_index, total_rows)\n",
    "                thread_data = all_data.iloc[start_index:actual_end_index].copy() # Pass a slice of the DataFrame\n",
    "                thread = threading.Thread(target=worker, args=((i + 1), thread_data, 0)) # Each thread gets its slice starting at index 0\n",
    "                threads.append(thread)\n",
    "                thread.start()\n",
    "            else:\n",
    "                print(f\"Warning: Start index {start_index} is out of bounds for thread {i + 1}\")\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        print(\"All threads have finished and saved their DataFrames.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780b0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\programming_projects\\RAG_fine_tune\\RAG_pipeline_ASU_website\\data\\chunked_cleaned_ASU_webpage_04_07_2025.csv\n",
      "Total rows: 400, using 3 threads.\n",
      "An error occurred: module 'threading' has no attribute 'Value'\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def worker(thread_id, all_data, row_counter, lock, rows_per_thread=100):\n",
    "    \"\"\"Worker function that continuously picks and processes chunks of the DataFrame.\"\"\"\n",
    "    while True:\n",
    "        with lock:\n",
    "            start_index = row_counter.value\n",
    "            row_counter.value += rows_per_thread\n",
    "            end_index = start_index + rows_per_thread\n",
    "\n",
    "        if start_index >= len(all_data):\n",
    "            print(f\"Thread {thread_id}: No more rows to process.\")\n",
    "            break\n",
    "\n",
    "        actual_end_index = min(end_index, len(all_data))\n",
    "        thread_data = all_data.iloc[start_index:actual_end_index].copy()\n",
    "        filename = f\"thread_{thread_id}_data_{start_index}.csv\"\n",
    "        thread_data.to_csv(filename, index=False)\n",
    "        print(f\"Thread {thread_id}: Processed rows {start_index} to {actual_end_index - 1} and saved to {filename}\")\n",
    "        time.sleep(0.1) # Optional: Add a small delay to avoid tight looping\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threads = []\n",
    "    num_threads = 3\n",
    "    rows_per_thread = 100\n",
    "\n",
    "    load_dotenv()\n",
    "    file_name = os.environ.get(\"file_name\")\n",
    "    directory_path = 'C:\\\\programming_projects\\\\RAG_fine_tune\\\\RAG_pipeline_ASU_website\\\\data\\\\'\n",
    "    file_path = directory_path + 'chunked_' + file_name\n",
    "    print(file_path)\n",
    "\n",
    "    try:\n",
    "        all_data = pd.read_csv(file_path, nrows=400)\n",
    "        total_rows = len(all_data)\n",
    "        print(f\"Total rows: {total_rows}, using {num_threads} threads.\")\n",
    "\n",
    "        # Shared counter to keep track of the next starting row\n",
    "        row_counter = threading.Value('i', 0)\n",
    "        lock = threading.Lock() # Lock to protect access to the counter\n",
    "\n",
    "        # Create and start the threads\n",
    "        for i in range(num_threads):\n",
    "            thread = threading.Thread(target=worker, args=(i + 1, all_data, row_counter, lock, rows_per_thread))\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        print(\"All threads have finished processing the data.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f6e2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: module 'threading' has no attribute 'Value'\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "try:\n",
    "    counter = threading.Value('i', 0)\n",
    "    print(\"threading.Value is available.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873adbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e62d9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement threading (from versions: none)\n",
      "ERROR: No matching distribution found for threading\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de46051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import re\n",
    "ASU_key = os.environ.get(\"ASU_key\")\n",
    "LLM_url = os.environ.get(\"LLM_url\")\n",
    "\n",
    "def cqa_api(chunked_df, i, ASU_key, LLM_url):\n",
    "    print(datetime.datetime.now())\n",
    "    df_out = pd.DataFrame([])\n",
    "    cleaned_string = chunked_df['cleaned_text'].loc[i]\n",
    "\n",
    "    chunk = chunked_df['chunked_text'].loc[i]\n",
    "    title = chunked_df['title'].loc[i]\n",
    "    url = chunked_df['url'].loc[i]\n",
    "    chunked_word_count = chunked_df['chunked_word_count'].loc[i]\n",
    "    orig_word_count = chunked_df['orig_word_count'].loc[i]\n",
    "    ##########################################\n",
    "    ## text type\n",
    "    bearer_token = ASU_key\n",
    "    json_payload = {\n",
    "        \"query\": \"what is the topic of the following text from: {cleaned_string}? only respond with the topic, no other text. Please make the topic 3 words or less\".format(cleaned_string=cleaned_string),\n",
    "        \"model_provider\": \"gcp-deepmind\",\n",
    "        \"model_name\": \"geminiflash2\",\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(LLM_url, headers=headers, json=json_payload)\n",
    "        response.raise_for_status()\n",
    "        result_document_section = response.json().get(\"response\")\n",
    "        # print(\"result:\", result_document_section)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    ##########################################\n",
    "    ## questions\n",
    "\n",
    "    query = \"\"\"given that the following text from the webpage {title} on url {url}, here is a text chunk limited to 500 words:\\n {chunk}\\n\\n \n",
    "                what are some good questions to ask about the text chunk? Please respond with a question and 3 different answers for each question.  There should be a total of 3 questions, with having 3 different answers (for a total of 9 unique answers).\n",
    "\n",
    "                the questions need to be well defined. Try to use the text as much as possible when crafting the answer. Answers need to be at least 2 sentences long. Do not use the phrase \"The text,\" and avoid similar language. Rephrase the question in the answer.\n",
    "\n",
    "                Please use the following format for the response:\n",
    "\n",
    "                **Question 1:**\n",
    "                **Question 1 Answer 1:**\n",
    "                **Question 1 Answer 2:**\n",
    "                **Question 1 Answer 3:**\n",
    "\n",
    "                **Question 2:**\n",
    "                **Question 2 Answer 1:**\n",
    "                **Question 2 Answer 2:**\n",
    "                **Question 2 Answer 3:**\n",
    "\n",
    "                **Question 3:**\n",
    "                **Question 3 Answer 1:**\n",
    "                **Question 3 Answer 2:**\n",
    "                **Question 3 Answer 3:**\n",
    "                \"\"\".format(\n",
    "                    title = title,\n",
    "                    url = url,\n",
    "                    chunk=chunk)\n",
    "\n",
    "    json_payload = {\n",
    "        \"query\": query,\n",
    "        \"model_provider\": \"gcp-deepmind\",\n",
    "        \"model_name\": \"geminiflash2\",\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(LLM_url, headers=headers, json=json_payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json().get(\"response\")\n",
    "        # print(\"result:\", result)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    \n",
    "    ###################################################\n",
    "    ## save out\n",
    "    parts = result.split(\"**\")  # Split the string at **\n",
    "    parts_no = [[2, 4], [2, 6], [2,8], [10, 12], [10, 14], [10, 16], [18, 20], [18, 22], [18, 24]]\n",
    "\n",
    "\n",
    "    for jj in range(0, 9):\n",
    "        try:\n",
    "            pt1 = parts_no[jj][0]\n",
    "            pt2 = parts_no[jj][1]\n",
    "\n",
    "            Question = re.sub(r'[^a-zA-Z0-9.,!?\\s]', ' ', str(parts[pt1]))\n",
    "            Question = Question.replace(r'\\s+', ' ').strip()\n",
    "            Answer = re.sub(r'[^a-zA-Z0-9.,!?\\s]', ' ', str(parts[pt2]))\n",
    "            Answer = Answer.replace(r'\\s+', ' ').strip()\n",
    "\n",
    "\n",
    "            Q1 = pd.DataFrame(data={'section':[result_document_section],\n",
    "                                    'title':[title],\n",
    "                                    'url': [url],\n",
    "                                    'document_type':['web page'],\n",
    "                                    'chunked_word_count':[chunked_word_count],\n",
    "                                    'orig_word_count':[orig_word_count],\n",
    "                                    'contex': [chunk],\n",
    "                                    'question':[Question],\n",
    "                                    'answer':[Answer]\n",
    "                                    })\n",
    "            df_out = pd.concat([Q1, df_out], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "    \n",
    "    return(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94d29d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-11 10:40:41.042222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>document_type</th>\n",
       "      <th>chunked_word_count</th>\n",
       "      <th>orig_word_count</th>\n",
       "      <th>contex</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>What makes  Suffs  a notable addition to the A...</td>\n",
       "      <td>Winning the Outer Critics Circle Award for Bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>What makes  Suffs  a notable addition to the A...</td>\n",
       "      <td>The musical is created by Shaina Taub, making ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>What makes  Suffs  a notable addition to the A...</td>\n",
       "      <td>Suffs  is a notable addition because it direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>Which shows are part of the ASU Gammage 2025 2...</td>\n",
       "      <td>The season also includes  Clue,  which runs fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>Which shows are part of the ASU Gammage 2025 2...</td>\n",
       "      <td>Another show featured in the season is    Juli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>Which shows are part of the ASU Gammage 2025 2...</td>\n",
       "      <td>The lineup includes  Suffs,  a Tony Award winn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>When do new season subscriptions for the ASU G...</td>\n",
       "      <td>Current season ticket holders have an advantag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>When do new season subscriptions for the ASU G...</td>\n",
       "      <td>Although subscriptions go on sale on April 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Broadway season announcement</td>\n",
       "      <td>ASU Gammage brings the best of Broadway to the...</td>\n",
       "      <td>https://news.asu.edu/20250225-arts-humanities-...</td>\n",
       "      <td>web page</td>\n",
       "      <td>500</td>\n",
       "      <td>989</td>\n",
       "      <td>ASU Gammage has announced its 2025 2026 Desert...</td>\n",
       "      <td>When do new season subscriptions for the ASU G...</td>\n",
       "      <td>New season subscriptions go on sale Monday, Ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        section  \\\n",
       "0  Broadway season announcement   \n",
       "1  Broadway season announcement   \n",
       "2  Broadway season announcement   \n",
       "3  Broadway season announcement   \n",
       "4  Broadway season announcement   \n",
       "5  Broadway season announcement   \n",
       "6  Broadway season announcement   \n",
       "7  Broadway season announcement   \n",
       "8  Broadway season announcement   \n",
       "\n",
       "                                               title  \\\n",
       "0  ASU Gammage brings the best of Broadway to the...   \n",
       "1  ASU Gammage brings the best of Broadway to the...   \n",
       "2  ASU Gammage brings the best of Broadway to the...   \n",
       "3  ASU Gammage brings the best of Broadway to the...   \n",
       "4  ASU Gammage brings the best of Broadway to the...   \n",
       "5  ASU Gammage brings the best of Broadway to the...   \n",
       "6  ASU Gammage brings the best of Broadway to the...   \n",
       "7  ASU Gammage brings the best of Broadway to the...   \n",
       "8  ASU Gammage brings the best of Broadway to the...   \n",
       "\n",
       "                                                 url document_type  \\\n",
       "0  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "1  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "2  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "3  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "4  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "5  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "6  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "7  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "8  https://news.asu.edu/20250225-arts-humanities-...      web page   \n",
       "\n",
       "   chunked_word_count  orig_word_count  \\\n",
       "0                 500              989   \n",
       "1                 500              989   \n",
       "2                 500              989   \n",
       "3                 500              989   \n",
       "4                 500              989   \n",
       "5                 500              989   \n",
       "6                 500              989   \n",
       "7                 500              989   \n",
       "8                 500              989   \n",
       "\n",
       "                                              contex  \\\n",
       "0  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "1  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "2  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "3  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "4  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "5  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "6  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "7  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "8  ASU Gammage has announced its 2025 2026 Desert...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What makes  Suffs  a notable addition to the A...   \n",
       "1  What makes  Suffs  a notable addition to the A...   \n",
       "2  What makes  Suffs  a notable addition to the A...   \n",
       "3  Which shows are part of the ASU Gammage 2025 2...   \n",
       "4  Which shows are part of the ASU Gammage 2025 2...   \n",
       "5  Which shows are part of the ASU Gammage 2025 2...   \n",
       "6  When do new season subscriptions for the ASU G...   \n",
       "7  When do new season subscriptions for the ASU G...   \n",
       "8  When do new season subscriptions for the ASU G...   \n",
       "\n",
       "                                              answer  \n",
       "0  Winning the Outer Critics Circle Award for Bes...  \n",
       "1  The musical is created by Shaina Taub, making ...  \n",
       "2  Suffs  is a notable addition because it direct...  \n",
       "3  The season also includes  Clue,  which runs fr...  \n",
       "4  Another show featured in the season is    Juli...  \n",
       "5  The lineup includes  Suffs,  a Tony Award winn...  \n",
       "6  Current season ticket holders have an advantag...  \n",
       "7  Although subscriptions go on sale on April 7, ...  \n",
       "8  New season subscriptions go on sale Monday, Ap...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(file_path, nrows=400)\n",
    "\n",
    "cqa_api(chunked_df = all_data, i = 399, ASU_key = ASU_key, LLM_url = LLM_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b5b3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\programming_projects\\RAG_fine_tune\\RAG_pipeline_ASU_website\\data\\chunked_cleaned_ASU_webpage_04_07_2025.csv\n",
      "Total rows: 400, using 3 processing threads and 3 writer threads.\n",
      "Thread 1: Processed rows 0 to 99 and added to queue.\n",
      "Thread 2: Processed rows 100 to 199 and added to queue.\n",
      "Thread 3: Processed rows 200 to 299 and added to queue.\n",
      "Writer Thread 1: Received and saved data to thread_1_data_0.csv\n",
      "Writer Thread 2: Received and saved data to thread_2_data_100.csv\n",
      "Writer Thread 3: Received and saved data to thread_3_data_200.csv\n",
      "Thread 1: Processed rows 300 to 399 and added to queue.\n",
      "All tasks in the queue have been processed by worker threads.\n",
      "Writer Thread 2: Received stop signal.\n",
      "Writer Thread 3: Received stop signal.\n",
      "Writer Thread 1: Received and saved data to thread_1_data_300.csv\n",
      "Writer Thread 1: Received stop signal.\n",
      "All writer threads have finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 3: Task queue is empty, exiting.\n",
      "Thread 2: Task queue is empty, exiting.\n",
      "Thread 1: Task queue is empty, exiting.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue, Empty\n",
    "\n",
    "def worker(thread_id, all_data, task_queue, output_queues):\n",
    "    \"\"\"Worker function that gets tasks from the queue and processes them.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            start_index, end_index = task_queue.get(timeout=1)\n",
    "            thread_data = all_data.iloc[start_index:end_index].copy()\n",
    "            output_queues[thread_id - 1].put((start_index, thread_data))  # Send start index with the data\n",
    "            print(f\"Thread {thread_id}: Processed rows {start_index} to {end_index - 1} and added to queue.\")\n",
    "            task_queue.task_done()\n",
    "            time.sleep(0.1)\n",
    "        except Empty:\n",
    "            print(f\"Thread {thread_id}: Task queue is empty, exiting.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Thread {thread_id}: An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "def writer_worker(queue, thread_id):\n",
    "    \"\"\"Worker function that takes dataframes and their start index from a queue and writes them to CSV files.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            start_index, thread_data = queue.get(timeout=1)\n",
    "            if thread_data.empty:  # Signal to stop\n",
    "                print(f\"Writer Thread {thread_id}: Received stop signal.\")\n",
    "                queue.task_done()\n",
    "                break\n",
    "            filename = f\"thread_{thread_id}_data_{start_index}.csv\"  # Use the received start_index\n",
    "            thread_data.to_csv(filename, index=False)\n",
    "            print(f\"Writer Thread {thread_id}: Received and saved data to {filename}\")\n",
    "            queue.task_done()\n",
    "        except Empty:\n",
    "            print(f\"Writer Thread {thread_id}: Writer queue empty, continuing to listen...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Writer Thread {thread_id}: An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threads = []\n",
    "    writer_threads = []\n",
    "    num_threads = 3\n",
    "    rows_per_thread = 100\n",
    "\n",
    "    load_dotenv()\n",
    "    file_name = os.environ.get(\"file_name\")\n",
    "    directory_path = 'C:\\\\programming_projects\\\\RAG_fine_tune\\\\RAG_pipeline_ASU_website\\\\data\\\\'\n",
    "    file_path = directory_path + 'chunked_' + file_name\n",
    "    print(file_path)\n",
    "\n",
    "    try:\n",
    "        all_data = pd.read_csv(file_path, nrows=400)\n",
    "        total_rows = len(all_data)\n",
    "        print(f\"Total rows: {total_rows}, using {num_threads} processing threads and {num_threads} writer threads.\")\n",
    "\n",
    "        task_queue = Queue()\n",
    "        output_queues = [Queue() for _ in range(num_threads)]\n",
    "\n",
    "        # Populate the task queue\n",
    "        num_chunks = (total_rows + rows_per_thread - 1) // rows_per_thread\n",
    "        for i in range(num_chunks):\n",
    "            start = i * rows_per_thread\n",
    "            end = min((i + 1) * rows_per_thread, total_rows)\n",
    "            task_queue.put((start, end))\n",
    "\n",
    "        # Create and start the processing threads\n",
    "        for i in range(num_threads):\n",
    "            thread = threading.Thread(target=worker, args=(i + 1, all_data, task_queue, output_queues))\n",
    "            threads.append(thread)\n",
    "            thread.daemon = True\n",
    "            thread.start()\n",
    "\n",
    "        # Create and start the writer threads\n",
    "        for i in range(num_threads):\n",
    "            writer_thread = threading.Thread(target=writer_worker, args=(output_queues[i], i + 1))\n",
    "            writer_threads.append(writer_thread)\n",
    "            writer_thread.daemon = True\n",
    "            writer_thread.start()\n",
    "\n",
    "        # Wait for all tasks to be processed\n",
    "        task_queue.join()\n",
    "        print(\"All tasks in the queue have been processed by worker threads.\")\n",
    "\n",
    "        # Signal writer threads to stop\n",
    "        for q in output_queues:\n",
    "            q.put((None, pd.DataFrame()))  # Send None for start_index as a stop signal\n",
    "\n",
    "        # Wait for writer threads to finish\n",
    "        for wt in writer_threads:\n",
    "            wt.join()\n",
    "        print(\"All writer threads have finished.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-11 11:40:36.643 C:\\programming_projects\\RAG_fine_tune\\RAG_pipeline_ASU_website\\data\\chunked_cleaned_ASU_webpage_04_07_2025.csv\n",
      "2025-04-11 11:40:36.646 Total rows: 35, using 3 processing threads and 3 writer threads.\n",
      "2025-04-11 11:40:36.647 Processing row index: 0\n",
      "2025-04-11 11:40:36.647 Processing row index: 1\n",
      "2025-04-11 11:40:36.648 Processing row index: 2\n",
      "2025-04-11 11:40:44.494 Thread 3: Processed rows 2 to 2 and added 9 rows to queue.\n",
      "2025-04-11 11:40:44.495 Writer Thread 3: Received 9 rows, now processed 1 original rows.\n",
      "2025-04-11 11:40:44.595 Processing row index: 3\n",
      "2025-04-11 11:40:45.134 Thread 2: Processed rows 1 to 1 and added 9 rows to queue.\n",
      "2025-04-11 11:40:45.135 Writer Thread 2: Received 9 rows, now processed 1 original rows.\n",
      "2025-04-11 11:40:45.235 Processing row index: 4\n",
      "2025-04-11 11:40:45.868 Thread 1: Processed rows 0 to 0 and added 9 rows to queue.\n",
      "2025-04-11 11:40:45.868 Writer Thread 1: Received 9 rows, now processed 1 original rows.\n",
      "2025-04-11 11:40:45.969 Processing row index: 5\n",
      "2025-04-11 11:40:52.170 Thread 3: Processed rows 3 to 3 and added 9 rows to queue.\n",
      "2025-04-11 11:40:52.171 Writer Thread 3: Received 9 rows, now processed 2 original rows.\n",
      "2025-04-11 11:40:52.271 Processing row index: 6\n",
      "2025-04-11 11:40:52.899 Thread 2: Processed rows 4 to 4 and added 9 rows to queue.\n",
      "2025-04-11 11:40:52.899 Writer Thread 2: Received 9 rows, now processed 2 original rows.\n",
      "2025-04-11 11:40:52.999 Processing row index: 7\n",
      "2025-04-11 11:40:53.651 Thread 1: Processed rows 5 to 5 and added 9 rows to queue.\n",
      "2025-04-11 11:40:53.653 Writer Thread 1: Received 9 rows, now processed 2 original rows.\n",
      "2025-04-11 11:40:53.754 Processing row index: 8\n",
      "2025-04-11 11:40:59.873 Thread 3: Processed rows 6 to 6 and added 9 rows to queue.\n",
      "2025-04-11 11:40:59.873 Writer Thread 3: Received 9 rows, now processed 3 original rows.\n",
      "2025-04-11 11:40:59.973 Processing row index: 9\n",
      "2025-04-11 11:41:00.114 Thread 2: Processed rows 7 to 7 and added 9 rows to queue.\n",
      "2025-04-11 11:41:00.114 Writer Thread 2: Received 9 rows, now processed 3 original rows.\n",
      "2025-04-11 11:41:00.215 Processing row index: 10\n",
      "2025-04-11 11:41:01.898 Thread 1: Processed rows 8 to 8 and added 9 rows to queue.\n",
      "2025-04-11 11:41:01.898 Writer Thread 1: Received 9 rows, now processed 3 original rows.\n",
      "2025-04-11 11:41:01.998 Processing row index: 11\n",
      "2025-04-11 11:41:07.624 Thread 3: Processed rows 9 to 9 and added 9 rows to queue.\n",
      "2025-04-11 11:41:07.624 Writer Thread 3: Received 9 rows, now processed 4 original rows.\n",
      "2025-04-11 11:41:07.725 Processing row index: 12\n",
      "2025-04-11 11:41:07.916 Thread 2: Processed rows 10 to 10 and added 9 rows to queue.\n",
      "2025-04-11 11:41:07.916 Writer Thread 2: Received 9 rows, now processed 4 original rows.\n",
      "2025-04-11 11:41:08.017 Processing row index: 13\n",
      "2025-04-11 11:41:09.681 Thread 1: Processed rows 11 to 11 and added 9 rows to queue.\n",
      "2025-04-11 11:41:09.682 Writer Thread 1: Received 9 rows, now processed 4 original rows.\n",
      "2025-04-11 11:41:09.782 Processing row index: 14\n",
      "2025-04-11 11:41:15.411 Thread 2: Processed rows 13 to 13 and added 9 rows to queue.\n",
      "2025-04-11 11:41:15.411 Writer Thread 2: Received 9 rows, now processed 5 original rows.\n",
      "2025-04-11 11:41:15.424 Thread 3: Processed rows 12 to 12 and added 9 rows to queue.\n",
      "2025-04-11 11:41:15.424 Writer Thread 3: Received 9 rows, now processed 5 original rows.\n",
      "2025-04-11 11:41:15.512 Processing row index: 15\n",
      "2025-04-11 11:41:15.524 Processing row index: 16\n",
      "2025-04-11 11:41:17.703 Thread 1: Processed rows 14 to 14 and added 9 rows to queue.\n",
      "2025-04-11 11:41:17.703 Writer Thread 1: Received 9 rows, now processed 5 original rows.\n",
      "2025-04-11 11:41:17.804 Processing row index: 17\n",
      "2025-04-11 11:41:23.133 Thread 2: Processed rows 15 to 15 and added 9 rows to queue.\n",
      "2025-04-11 11:41:23.133 Writer Thread 2: Received 9 rows, now processed 6 original rows.\n",
      "2025-04-11 11:41:23.234 Processing row index: 18\n",
      "2025-04-11 11:41:23.312 Thread 3: Processed rows 16 to 16 and added 9 rows to queue.\n",
      "2025-04-11 11:41:23.313 Writer Thread 3: Received 9 rows, now processed 6 original rows.\n",
      "2025-04-11 11:41:23.413 Processing row index: 19\n",
      "2025-04-11 11:41:26.290 Thread 1: Processed rows 17 to 17 and added 9 rows to queue.\n",
      "2025-04-11 11:41:26.290 Writer Thread 1: Received 9 rows, now processed 6 original rows.\n",
      "2025-04-11 11:41:26.390 Processing row index: 20\n",
      "2025-04-11 11:41:31.085 Thread 3: Processed rows 19 to 19 and added 9 rows to queue.\n",
      "2025-04-11 11:41:31.085 Writer Thread 3: Received 9 rows, now processed 7 original rows.\n",
      "2025-04-11 11:41:31.186 Processing row index: 21\n",
      "2025-04-11 11:41:31.475 Thread 2: Processed rows 18 to 18 and added 9 rows to queue.\n",
      "2025-04-11 11:41:31.476 Writer Thread 2: Received 9 rows, now processed 7 original rows.\n",
      "2025-04-11 11:41:31.576 Processing row index: 22\n",
      "2025-04-11 11:41:33.950 Thread 1: Processed rows 20 to 20 and added 9 rows to queue.2025-04-11 11:41:33.950 Writer Thread 1: Received 9 rows, now processed 7 original rows.\n",
      "\n",
      "2025-04-11 11:41:34.051 Processing row index: 23\n",
      "2025-04-11 11:41:38.945 Thread 3: Processed rows 21 to 21 and added 9 rows to queue.\n",
      "2025-04-11 11:41:38.946 Writer Thread 3: Received 9 rows, now processed 8 original rows.\n",
      "2025-04-11 11:41:39.046 Processing row index: 24\n",
      "2025-04-11 11:41:39.444 Thread 2: Processed rows 22 to 22 and added 9 rows to queue.\n",
      "2025-04-11 11:41:39.444 Writer Thread 2: Received 9 rows, now processed 8 original rows.\n",
      "2025-04-11 11:41:39.545 Processing row index: 25\n",
      "2025-04-11 11:41:42.885 Thread 1: Processed rows 23 to 23 and added 9 rows to queue.\n",
      "2025-04-11 11:41:42.885 Writer Thread 1: Received 9 rows, now processed 8 original rows.\n",
      "2025-04-11 11:41:42.986 Processing row index: 26\n",
      "2025-04-11 11:41:46.305 Thread 3: Processed rows 24 to 24 and added 9 rows to queue.\n",
      "2025-04-11 11:41:46.305 Writer Thread 3: Received 9 rows, now processed 9 original rows.\n",
      "2025-04-11 11:41:46.406 Processing row index: 27\n",
      "2025-04-11 11:41:46.566 Thread 2: Processed rows 25 to 25 and added 9 rows to queue.2025-04-11 11:41:46.566 Writer Thread 2: Received 9 rows, now processed 9 original rows.\n",
      "\n",
      "2025-04-11 11:41:46.667 Processing row index: 28\n",
      "2025-04-11 11:41:51.441 Thread 1: Processed rows 26 to 26 and added 9 rows to queue.\n",
      "2025-04-11 11:41:51.442 Writer Thread 1: Received 9 rows, now processed 9 original rows.\n",
      "2025-04-11 11:41:51.543 Processing row index: 29\n",
      "2025-04-11 11:41:53.567 Thread 3: Processed rows 27 to 27 and added 9 rows to queue.\n",
      "2025-04-11 11:41:53.567 Writer Thread 3: Received 9 rows, now processed 10 original rows.\n",
      "2025-04-11 11:41:53.569 Writer Thread 3: Saved 90 rows (from 10 original) to processed_data_thread_3_start_2_10rows.csv.\n",
      "2025-04-11 11:41:53.667 Processing row index: 30\n",
      "2025-04-11 11:41:54.389 Thread 2: Processed rows 28 to 28 and added 9 rows to queue.\n",
      "2025-04-11 11:41:54.389 Writer Thread 2: Received 9 rows, now processed 10 original rows.\n",
      "2025-04-11 11:41:54.391 Writer Thread 2: Saved 90 rows (from 10 original) to processed_data_thread_2_start_1_10rows.csv.\n",
      "2025-04-11 11:41:54.490 Processing row index: 31\n",
      "2025-04-11 11:41:59.464 Thread 1: Processed rows 29 to 29 and added 9 rows to queue.\n",
      "2025-04-11 11:41:59.464 Writer Thread 1: Received 9 rows, now processed 10 original rows.\n",
      "2025-04-11 11:41:59.469 Writer Thread 1: Saved 90 rows (from 10 original) to processed_data_thread_1_start_0_10rows.csv.\n",
      "2025-04-11 11:41:59.565 Processing row index: 32\n",
      "2025-04-11 11:42:01.233 Thread 3: Processed rows 30 to 30 and added 9 rows to queue.\n",
      "2025-04-11 11:42:01.234 Writer Thread 3: Received 9 rows, now processed 1 original rows.\n",
      "2025-04-11 11:42:01.334 Processing row index: 33\n",
      "2025-04-11 11:42:01.881 Thread 2: Processed rows 31 to 31 and added 9 rows to queue.2025-04-11 11:42:01.881 Writer Thread 2: Received 9 rows, now processed 1 original rows.\n",
      "\n",
      "2025-04-11 11:42:01.982 Processing row index: 34\n",
      "2025-04-11 11:42:06.851 Thread 1: Processed rows 32 to 32 and added 9 rows to queue.2025-04-11 11:42:06.851 Writer Thread 1: Received 9 rows, now processed 1 original rows.\n",
      "\n",
      "2025-04-11 11:42:07.962 Thread 1: Task queue is empty, exiting.\n",
      "2025-04-11 11:42:08.617 Thread 3: Processed rows 33 to 33 and added 9 rows to queue.\n",
      "2025-04-11 11:42:08.617 Writer Thread 3: Received 9 rows, now processed 2 original rows.\n",
      "2025-04-11 11:42:09.612 Thread 2: Processed rows 34 to 34 and added 9 rows to queue.\n",
      "2025-04-11 11:42:09.613 All tasks in the queue have been processed by worker threads.\n",
      "2025-04-11 11:42:09.613 Writer Thread 2: Received 9 rows, now processed 2 original rows.\n",
      "2025-04-11 11:42:09.613 Writer Thread 2: Received stop signal. Processed 2 original rows.\n",
      "2025-04-11 11:42:09.613 Writer Thread 3: Received stop signal. Processed 2 original rows.\n",
      "2025-04-11 11:42:09.613 Writer Thread 1: Received stop signal. Processed 1 original rows.\n",
      "2025-04-11 11:42:09.615 Writer Thread 2: Saved remaining 18 rows to processed_data_thread_2_start_31_partial.csv.\n",
      "2025-04-11 11:42:09.615 Writer Thread 1: Saved remaining 9 rows to processed_data_thread_1_start_32_partial.csv.\n",
      "2025-04-11 11:42:09.615 Writer Thread 3: Saved remaining 18 rows to processed_data_thread_3_start_30_partial.csv.\n",
      "2025-04-11 11:42:09.615 All writer threads have finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-11 11:42:09.730 Thread 3: Task queue is empty, exiting.\n",
      "2025-04-11 11:42:10.724 Thread 2: Task queue is empty, exiting.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue, Empty\n",
    "import datetime\n",
    "import requests\n",
    "import re\n",
    "\n",
    "ASU_key = os.environ.get(\"ASU_key\")\n",
    "LLM_url = os.environ.get(\"LLM_url\")\n",
    "\n",
    "def make_llm_request(query, api_key, api_url):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    json_payload = {\"query\": query, \"model_provider\": \"gcp-deepmind\", \"model_name\": \"geminiflash2\"}\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=json_payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"response\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} API request error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_cqa_response(llm_response, title, url, chunked_word_count, orig_word_count, chunk):\n",
    "    df_out = pd.DataFrame([])\n",
    "    if llm_response:\n",
    "        parts = llm_response.split(\"**\")\n",
    "        parts_no = [[2, 4], [2, 6], [2,8], [10, 12], [10, 14], [10, 16], [18, 20], [18, 22], [18, 24]]\n",
    "        for jj in range(0, 9):\n",
    "            try:\n",
    "                pt1 = parts_no[jj][0]\n",
    "                pt2 = parts_no[jj][1]\n",
    "                Question = re.sub(r'[^a-zA-Z0-9.,!?\\s]', ' ', str(parts[pt1])).replace(r'\\s+', ' ').strip()\n",
    "                Answer = re.sub(r'[^a-zA-Z0-9.,!?\\s]', ' ', str(parts[pt2])).replace(r'\\s+', ' ').strip()\n",
    "                Q1 = pd.DataFrame(data={'title':[title], 'url': [url], 'document_type':['web page'], 'chunked_word_count':[chunked_word_count], 'orig_word_count':[orig_word_count], 'contex': [chunk], 'question':[Question], 'answer':[Answer]})\n",
    "                df_out = pd.concat([Q1, df_out], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                print(f\"{timestamp} Error parsing response: {e}\")\n",
    "    return df_out\n",
    "\n",
    "def cqa_api(chunked_df, i, ASU_key, LLM_url):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    print(f\"{timestamp} Processing row index: {i}\")\n",
    "    chunk = chunked_df['chunked_text'].iloc[i]\n",
    "    title = chunked_df['title'].iloc[i]\n",
    "    url = chunked_df['url'].iloc[i]\n",
    "    chunked_word_count = chunked_df['chunked_word_count'].iloc[i]\n",
    "    orig_word_count = chunked_df['orig_word_count'].iloc[i]\n",
    "\n",
    "    qa_query = f\"\"\"given that the following text from the webpage {title} on url {url}, here is a text chunk limited to 500 words:\\n {chunk}\\n\\n what are some good questions to ask about the text chunk? Please respond with a question and 3 different answers for each question. There should be a total of 3 questions, with having 3 different answers (for a total of 9 unique answers). ... (rest of the prompt)\"\"\"\n",
    "    llm_response = make_llm_request(qa_query, ASU_key, LLM_url)\n",
    "\n",
    "    return parse_cqa_response(llm_response, title, url, chunked_word_count, orig_word_count, chunk)\n",
    "\n",
    "def worker(thread_id, all_data, task_queue, output_queues):\n",
    "    \"\"\"Worker function that gets tasks from the queue and processes them.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            start_index, end_index = task_queue.get(timeout=1)\n",
    "            thread_data_chunk = all_data.iloc[start_index:end_index].copy()\n",
    "            all_processed_dfs = []\n",
    "            for i in thread_data_chunk.index: # Iterate through the rows of the chunk\n",
    "                output_df = cqa_api(all_data, i, ASU_key, LLM_url)\n",
    "                if not output_df.empty:\n",
    "                    all_processed_dfs.append(output_df)\n",
    "\n",
    "            if all_processed_dfs:\n",
    "                combined_df = pd.concat(all_processed_dfs, ignore_index=True)\n",
    "                output_queues[thread_id - 1].put((start_index, combined_df))\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                print(f\"{timestamp} Thread {thread_id}: Processed rows {start_index} to {end_index - 1} and added {len(combined_df)} rows to queue.\")\n",
    "            else:\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                print(f\"{timestamp} Thread {thread_id}: Processed rows {start_index} to {end_index - 1}, no output generated.\")\n",
    "\n",
    "            task_queue.task_done()\n",
    "            time.sleep(0.1)\n",
    "        except Empty:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "            print(f\"{timestamp} Thread {thread_id}: Task queue is empty, exiting.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "            print(f\"{timestamp} Thread {thread_id}: An error occurred in worker: {e}\")\n",
    "            break\n",
    "\n",
    "def writer_worker(queue, thread_id, all_data):\n",
    "    \"\"\"Worker function that takes dataframes and their start index from a queue and appends to a local dataframe until 10 original rows are processed, then writes to CSV.\"\"\"\n",
    "    local_df = pd.DataFrame()\n",
    "    processed_original_rows = set()\n",
    "    while True:\n",
    "        try:\n",
    "            start_index, thread_data = queue.get(timeout=1)\n",
    "            if thread_data.empty:\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                print(f\"{timestamp} Writer Thread {thread_id}: Received stop signal. Processed {len(processed_original_rows)} original rows.\")\n",
    "                break\n",
    "\n",
    "            local_df = pd.concat([local_df, thread_data], ignore_index=True)\n",
    "            for i in range(start_index, start_index + (len(thread_data) // 9)): # Assuming 9 output rows per input row\n",
    "                processed_original_rows.add(i)\n",
    "\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "            print(f\"{timestamp} Writer Thread {thread_id}: Received {len(thread_data)} rows, now processed {len(processed_original_rows)} original rows.\")\n",
    "            queue.task_done()\n",
    "\n",
    "            if len(processed_original_rows) >= 10:\n",
    "                output_start_index = min(processed_original_rows)\n",
    "                filename = f\"processed_data_thread_{thread_id}_start_{output_start_index}_10rows.csv\"\n",
    "                local_df.to_csv(filename, index=False)\n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                print(f\"{timestamp} Writer Thread {thread_id}: Saved {len(local_df)} rows (from 10 original) to {filename}.\")\n",
    "                local_df = pd.DataFrame()\n",
    "                processed_original_rows = set()\n",
    "\n",
    "        except Empty:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "            print(f\"{timestamp} Writer Thread {thread_id}: An error occurred in writer: {e}\")\n",
    "            break\n",
    "\n",
    "    # Save any remaining data if the stop signal is received before 10 rows\n",
    "    if not local_df.empty:\n",
    "        output_start_index = min(processed_original_rows) if processed_original_rows else \"partial\"\n",
    "        filename = f\"processed_data_thread_{thread_id}_start_{output_start_index}_partial.csv\"\n",
    "        local_df.to_csv(filename, index=False)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} Writer Thread {thread_id}: Saved remaining {len(local_df)} rows to {filename}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threads = []\n",
    "    writer_threads = []\n",
    "    num_threads = 3\n",
    "    rows_per_thread = 1  # Process one row at a time with the API call\n",
    "    original_rows_per_writer_file = 10\n",
    "\n",
    "    load_dotenv()\n",
    "    file_name = os.environ.get(\"file_name\")\n",
    "    directory_path = 'C:\\\\programming_projects\\\\RAG_fine_tune\\\\RAG_pipeline_ASU_website\\\\data\\\\'\n",
    "    file_path = directory_path + 'chunked_' + file_name\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    print(f\"{timestamp} {file_path}\")\n",
    "\n",
    "    try:\n",
    "        all_data = pd.read_csv(file_path, nrows=35) # Increased nrows for testing\n",
    "        total_rows = len(all_data)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} Total rows: {total_rows}, using {num_threads} processing threads and {num_threads} writer threads.\")\n",
    "\n",
    "        task_queue = Queue()\n",
    "        output_queues = [Queue() for _ in range(num_threads)]\n",
    "\n",
    "        # Populate the task queue with individual row indices\n",
    "        for i in range(total_rows):\n",
    "            task_queue.put((i, i + 1)) # Each task is to process a single row\n",
    "\n",
    "        # Create and start the processing threads\n",
    "        for i in range(num_threads):\n",
    "            thread = threading.Thread(target=worker, args=(i + 1, all_data, task_queue, output_queues))\n",
    "            threads.append(thread)\n",
    "            thread.daemon = True\n",
    "            thread.start()\n",
    "\n",
    "        # Create and start the writer threads\n",
    "        for i in range(num_threads):\n",
    "            writer_thread = threading.Thread(target=writer_worker, args=(output_queues[i], i + 1, all_data))\n",
    "            writer_threads.append(writer_thread)\n",
    "            writer_thread.daemon = True\n",
    "            writer_thread.start()\n",
    "\n",
    "        # Wait for all tasks to be processed\n",
    "        task_queue.join()\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} All tasks in the queue have been processed by worker threads.\")\n",
    "\n",
    "        # Signal writer threads to stop\n",
    "        for q in output_queues:\n",
    "            q.put((None, pd.DataFrame()))\n",
    "\n",
    "        # Wait for writer threads to finish\n",
    "        for wt in writer_threads:\n",
    "            wt.join()\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} All writer threads have finished.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "        print(f\"{timestamp} An error occurred in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd2d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading 1_chunk_txt.ipynb: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
      "\n",
      "Error reading 1_chunk_txt.py: Error tokenizing data. C error: Expected 1 fields in line 13, saw 3\n",
      "\n",
      "Error reading 2_question_answer_context.ipynb: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
      "\n",
      "Error reading 2_question_answer_context.py: Error tokenizing data. C error: Expected 1 fields in line 26, saw 2\n",
      "\n",
      "Error reading 3_cqa_multi.ipynb: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
      "\n",
      "Error reading data: [Errno 13] Permission denied: 'C:\\\\programming_projects\\\\RAG_fine_tune\\\\RAG_pipeline_ASU_website\\\\data'\n",
      "processed_data_thread_1_start_0_10rows.csv (90, 8)\n",
      "processed_data_thread_1_start_32_partial.csv (9, 8)\n",
      "processed_data_thread_2_start_1_10rows.csv (90, 8)\n",
      "processed_data_thread_2_start_31_partial.csv (18, 8)\n",
      "processed_data_thread_3_start_2_10rows.csv (90, 8)\n",
      "processed_data_thread_3_start_30_partial.csv (18, 8)\n",
      "(315, 9)\n",
      "drop dups page_text: (315, 9)\n"
     ]
    }
   ],
   "source": [
    "def files_to_dataframe(directory):\n",
    "    all_data = pd.DataFrame([])\n",
    "    for file in os.listdir(directory):\n",
    "        try:\n",
    "            df_read = pd.read_csv(directory+file, encoding=\"utf_8_sig\", low_memory=False)\n",
    "            print(file, df_read.shape)\n",
    "            df_read['filename'] = str(file)\n",
    "            # df_read.drop_duplicates(inplace=True)\n",
    "            ##print(file, df_read.shape)\n",
    "            all_data = pd.concat([all_data, df_read])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    return all_data\n",
    "\n",
    "directory_path = 'C:\\\\programming_projects\\\\RAG_fine_tune\\\\RAG_pipeline_ASU_website\\\\'\n",
    "df = files_to_dataframe(directory_path)\n",
    "print( df.shape)\n",
    "df.drop_duplicates(subset = ['answer'], inplace=True)\n",
    "print('drop dups page_text:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6b5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
