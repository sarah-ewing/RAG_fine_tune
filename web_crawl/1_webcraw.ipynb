{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: readability-lxml in c:\\users\\onyxs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.8.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\onyxs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from readability-lxml) (5.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\onyxs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from readability-lxml) (5.3.1)\n",
      "Requirement already satisfied: cssselect in c:\\users\\onyxs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from readability-lxml) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\onyxs\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install readability-lxml\n",
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time\n",
    "import threading\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from transformers import pipeline\n",
    "\n",
    "# User-Agent Header to Mimic a Real Browser\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "REQUEST_DELAY = 1 # Seconds between requests\n",
    "\n",
    "# Zero-Shot Classification Model\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# List of Topics for Classification\n",
    "TOPIC_CATEGORIES = [\n",
    "    \"Academic Programs & Courses\",\n",
    "    \"Admissions & Application Process\",\n",
    "    \"Scholarships & Financial Aid\",\n",
    "    \"Research & Innovation at ASU\",\n",
    "    \"Student Life & Campus Activities\",\n",
    "    \"ASU's Global & Online Education\",\n",
    "    \"ASU's Commitment to Sustainability\",\n",
    "    \"International Student Support\",\n",
    "    \"ASU's AI & Tech Initiatives\",\n",
    "    \"Sun Devil Athletics & Sports\",\n",
    "    \"ASU's History & Rankings\",\n",
    "    \"ASU Library & Research Resources\",\n",
    "    \"Career Services & Job Support\",\n",
    "    \"Housing & Campus Life\",\n",
    "    \"Health, Wellness & Counseling Services\",\n",
    "    \"ASU Welbeing and Security\"\n",
    "]\n",
    "\n",
    "##################################################################\n",
    "from readability import Document\n",
    "\n",
    "def extract_main_content(html_content):\n",
    "    \"\"\"Extracts the main readable content from HTML using readability-lxml.\"\"\"\n",
    "    try:\n",
    "        doc = Document(html_content)\n",
    "        summary = doc.summary()\n",
    "        # Remove HTML tags from the summary\n",
    "        text = html.fromstring(summary).text_content().strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        return \"\"\n",
    "##################################################################\n",
    "def classify_topic(text):\n",
    "    \"\"\"Classifies webpage topic using Zero-Shot Learning.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    try:\n",
    "        result = classifier(text, TOPIC_CATEGORIES)\n",
    "        return result[\"labels\"][0]  # Top predicted category\n",
    "    except Exception as e:\n",
    "        print(f\"Error in topic classification: {e}\")\n",
    "        return \"Unknown\"\n",
    "##################################################################\n",
    "def get_url_tree(start_url, max_depth=3):\n",
    "    \"\"\"\n",
    "    Crawls a website up to a given depth while respecting robots.txt and rate limits.\n",
    "    \n",
    "    Args:\n",
    "        start_url (str): The initial URL to start crawling.\n",
    "        max_depth (int): Maximum depth of crawling.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing URL data.\n",
    "    \"\"\"\n",
    "    visited = set()  \n",
    "    results = []  # Stores results \n",
    "    url_queue = deque([(start_url, 0)])  # Queue for BFS crawling\n",
    "\n",
    "    while url_queue:\n",
    "        url, depth = url_queue.popleft()\n",
    "\n",
    "        # Stop if max depth is reached\n",
    "        if depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        # Skip already visited URLs\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()  # Raise error for bad responses\n",
    "            tree = html.fromstring(response.text)\n",
    "\n",
    "            # Extract meaningful content\n",
    "            page_text = extract_main_content(response.text) # pass the text, not the tree.\n",
    "            num_words = len(page_text.split())\n",
    "            num_chars = len(page_text)\n",
    "\n",
    "            # Extract title\n",
    "            title_element = tree.xpath('//title/text()')\n",
    "            title = title_element[0].strip() if title_element else \"Untitled\"\n",
    "\n",
    "            # Classify topic\n",
    "            topic = classify_topic(page_text)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'url': url,\n",
    "                'depth': depth,\n",
    "                'title': title,\n",
    "                'topic': topic,\n",
    "                'word_count': num_words,\n",
    "                'char_count': num_chars,\n",
    "                'page_text': page_text \n",
    "            })\n",
    "\n",
    "            # Extract and queue new links\n",
    "            links = tree.xpath('//a/@href')\n",
    "            for link in links:\n",
    "                absolute_link = urljoin(url, link)  # Convert to absolute URL\n",
    "                parsed_link = urlparse(absolute_link)\n",
    "                \n",
    "                # Only follow links within the same domain\n",
    "                if parsed_link.netloc == urlparse(start_url).netloc and \"#\" not in absolute_link and can_fetch(absolute_link):\n",
    "                    url_queue.append((absolute_link, depth + 1))\n",
    "\n",
    "            # Delay\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error accessing {url}: {e}\")\n",
    "\n",
    "    return results\n",
    "##################################################################\n",
    "def ticker(results, interval):\n",
    "    \"\"\"Displays a live ticker showing the number of URLs processed.\"\"\"\n",
    "    start_time = time.time()\n",
    "    last_count = 0\n",
    "    while True:\n",
    "        current_count = len(results)\n",
    "        if current_count != last_count:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Processed {current_count} URLs. Elapsed time: {elapsed_time:.2f} seconds\", end='\\r')\n",
    "            last_count = current_count\n",
    "        time.sleep(interval)\n",
    "\n",
    "##################################################################\n",
    "def can_fetch(url, user_agent=\"*\"):\n",
    "    \"\"\"Checks if a URL can be fetched according to robots.txt.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp.can_fetch(user_agent, url)\n",
    "    except:\n",
    "        return True # if robots.txt is unavailable, assume we can crawl.\n",
    "##################################################################\n",
    "def main():\n",
    "    start_url = \"https://www.asu.edu/\"\n",
    "    max_depth = 9 # Limit the depth of crawling\n",
    "    results = []\n",
    "\n",
    "    # Start the ticker thread\n",
    "    ticker_thread = threading.Thread(target=ticker, args=(results, 2))\n",
    "    ticker_thread.daemon = True\n",
    "    ticker_thread.start()\n",
    "\n",
    "    # Start crawling\n",
    "    url_data = get_url_tree(start_url, max_depth=max_depth)\n",
    "\n",
    "    # timer\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(url_data)\n",
    "\n",
    "    # Save results to CSV\n",
    "    print(\"\\n\\nFinal Results:\")\n",
    "    print(df[['url', 'depth', 'title', 'topic', 'word_count', 'char_count']])\n",
    "    df.to_csv(\"webpage_analysis_v2.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
